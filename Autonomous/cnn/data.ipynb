{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impossible-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Concatenate, UpSampling2D, Add\n",
    "from tensorflow.keras.layers import Convolution2D, Dense, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fancy-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1622419511952088.jpg', '1622419506502318.jpg', '1622419528268973.jpg', '1622419576046052.jpg', '1622419527177214.jpg', '1622419614871980.jpg', '1622419533705117.jpg', '1622419514156086.jpg', '1622419530437773.jpg', '1622419531525838.jpg', '1622419505408716.jpg', '1622419613773041.jpg', '1622419612683097.jpg', '1622419523935783.jpg', '1622419518489591.jpg', '1622419509784848.jpg', '1622419532613964.jpg', '1622419610516969.jpg', '1622419520677765.jpg', '1622419519582132.jpg', '1622419507617507.jpg']\n"
     ]
    }
   ],
   "source": [
    "ins = []\n",
    "outs = []\n",
    "imDir = 'images'\n",
    "labelDir = 'labels'\n",
    "\n",
    "filt = '*'\n",
    "#print(os.listdir(imDir))\n",
    "files = [f for f in os.listdir(imDir) if re.match(r'.%s.jpg'%filt, f)]\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revolutionary-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = []\n",
    "outs = []\n",
    "for f in files:\n",
    "    imFile = imDir + '/' + f\n",
    "    labFile = labelDir + '/' + f\n",
    "    \n",
    "    im = Image.open(imFile)\n",
    "    ins.append(np.asarray(im))\n",
    "    \n",
    "    label = Image.open(labFile)\n",
    "    label = np.asarray(label)\n",
    "    outs.append(np.where(label > 100, 1, 0))\n",
    "    \n",
    "sh = ins[0].shape\n",
    "newSh = (1, sh[0], sh[1], sh[2])\n",
    "ins = [np.reshape(i, newshape = newSh) for i in ins]\n",
    "ins = np.concatenate(ins, axis=0)\n",
    "\n",
    "sh = outs[0].shape\n",
    "newSh = (1, sh[0], sh[1])\n",
    "outs = [np.reshape(o, newshape = newSh) for o in outs]\n",
    "outs = np.concatenate(outs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informed-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(inputShape):\n",
    "    inputTensor = Input(shape = inputShape, name = 'input')\n",
    "    \n",
    "    tensors = []\n",
    "    previousTensor = inputTensor\n",
    "    count = 0\n",
    "    \n",
    "    tensors.append(Convolution2D(\n",
    "            filters = 20,\n",
    "            kernel_size = 3,\n",
    "            strides = 3,\n",
    "            padding = 'same',\n",
    "            activation = 'elu',\n",
    "            name = '1')(previousTensor))\n",
    "    previousTensor = tensors[0]\n",
    "    \n",
    "    tensors.append(Convolution2D(\n",
    "            filters = 40,\n",
    "            kernel_size = 3,\n",
    "            strides = 4,\n",
    "            padding = 'same',\n",
    "            activation = 'elu',\n",
    "            name = '2')(previousTensor))\n",
    "    previousTensor = tensors[1]\n",
    "        \n",
    "    tensors.append(Convolution2D(\n",
    "            filters = 60,\n",
    "            kernel_size = 3,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "            activation = 'elu',\n",
    "            name = '3')(previousTensor))\n",
    "    previousTensor = tensors[2]\n",
    "    \n",
    "    previousTensor = UpSampling2D(size = 24)(previousTensor)\n",
    "    \n",
    "    outputTensor = Convolution2D(2,\n",
    "                            kernel_size = (1,1),\n",
    "                            padding='same',\n",
    "                            name = 'output',\n",
    "                            activation='softmax')(previousTensor)\n",
    "    \n",
    "    #creates the model\n",
    "    model = Model(inputs = inputTensor, outputs = outputTensor)\n",
    "    \n",
    "    #creates the optimizer\n",
    "    opt = tf.keras.optimizers.Adam(lr=.0001, beta_1=.9, beta_2=.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    #compiles, prints and returns the model\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=opt,\n",
    "                 metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "other-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 1080, 1920, 3)\n",
      "(21, 1080, 1920)\n"
     ]
    }
   ],
   "source": [
    "print(ins.shape)\n",
    "print(outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "valuable-tuition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1080, 1920, 3)]   0         \n",
      "_________________________________________________________________\n",
      "1 (Conv2D)                   (None, 360, 640, 20)      560       \n",
      "_________________________________________________________________\n",
      "2 (Conv2D)                   (None, 90, 160, 40)       7240      \n",
      "_________________________________________________________________\n",
      "3 (Conv2D)                   (None, 45, 80, 60)        21660     \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 1080, 1920, 60)    0         \n",
      "_________________________________________________________________\n",
      "output (Conv2D)              (None, 1080, 1920, 2)     122       \n",
      "=================================================================\n",
      "Total params: 29,582\n",
      "Trainable params: 29,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = createModel(ins.shape[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southwest-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set_generator_images(ins, outs, batch_size=10,\n",
    "                          input_name='input', \n",
    "                        output_name='output'):\n",
    "    '''\n",
    "    Generator for producing random minibatches of image training samples.\n",
    "    \n",
    "    @param ins Full set of training set inputs (examples x row x col x chan)\n",
    "    @param outs Corresponding set of sample (examples x nclasses)\n",
    "    @param batch_size Number of samples for each minibatch\n",
    "    @param input_name Name of the model layer that is used for the input of the model\n",
    "    @param output_name Name of the model layer that is used for the output of the model\n",
    "    '''\n",
    "    \n",
    "    while True:\n",
    "        # Randomly select a set of example indices\n",
    "        example_indices = random.choices(range(ins.shape[0]), k=batch_size)\n",
    "        \n",
    "        # The generator will produce a pair of return values: one for inputs and one for outputs\n",
    "        yield({input_name: ins[example_indices,:,:,:]},\n",
    "             {output_name: outs[example_indices,:,:]})\n",
    "        \n",
    "generator = training_set_generator_images(ins, outs, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocal-tracker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 2.6522 - sparse_categorical_accuracy: 0.6645\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 1.5374 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.8021 - sparse_categorical_accuracy: 0.8569\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.4305 - sparse_categorical_accuracy: 0.9198\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.2556 - sparse_categorical_accuracy: 0.9547\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.1720 - sparse_categorical_accuracy: 0.9703\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9793\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.1037 - sparse_categorical_accuracy: 0.9846\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0799 - sparse_categorical_accuracy: 0.9895\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0736 - sparse_categorical_accuracy: 0.9905\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9914\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9922\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9928\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9936\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9939\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0578 - sparse_categorical_accuracy: 0.9942\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0570 - sparse_categorical_accuracy: 0.9944\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0564 - sparse_categorical_accuracy: 0.9945\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9947\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9949\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9950\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9951\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0544 - sparse_categorical_accuracy: 0.9952\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9954\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0539 - sparse_categorical_accuracy: 0.9954\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0537 - sparse_categorical_accuracy: 0.9955\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9955\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9955\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0532 - sparse_categorical_accuracy: 0.9956\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9956\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0528 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0525 - sparse_categorical_accuracy: 0.9957\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0524 - sparse_categorical_accuracy: 0.9958\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9958\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9959\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0521 - sparse_categorical_accuracy: 0.9959\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0521 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0520 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0519 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9961\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9961\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9961\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0516 - sparse_categorical_accuracy: 0.9961\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9962\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9963\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0508 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0508 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0508 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0506 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0506 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0506 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0505 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0505 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0505 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9966\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0498 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.0498 - sparse_categorical_accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=ins, y=outs, epochs=100, steps_per_epoch = 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continued-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fitting-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.argmax(pred, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "amber-orbit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6e2c6e60d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(labels[10,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "medieval-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6e2c74ffa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(outs[7,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ins[4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "convertible-centre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: testModel/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('testModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "assumed-consolidation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "advance-python",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-b14b700e5bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpb_filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#save(tf.train.Saver())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Saver'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
